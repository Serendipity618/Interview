# ML Apprentice Take-Home Project

This project implements a multi-task sentence transformer using BERT (`bert-base-uncased`), following the ML Apprentice take-home exercise requirements. It is structured for clarity and ease of reproducibility.

---

## ğŸ§  Tasks Implemented

### âœ… Task 1: Sentence Transformer
- Uses a pre-trained `bert-base-uncased` model.
- Applies mean pooling to get fixed-length sentence embeddings.

### âœ… Task 2: Multi-Task Learning
- Two task-specific heads:
  - **Task A**: Sentence classification (e.g., topic)
  - **Task B**: Sentiment analysis

### âœ… Task 3: Training Considerations
- Freezing of the transformer encoder is supported for transfer learning.
- Optionally allows fine-tuning the encoder.

### âœ… Task 4: Training Loop
- One-step training loop with support for multiple epochs.
- Supports joint optimization of multi-task losses.

---

## ğŸ“‚ Project Structure

```
Interview/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ model.py         # Multi-task sentence transformer (Task 1 & 2)
â”‚   â”œâ”€â”€ utils.py         # Training loop with epoch support (Task 4)
â”‚   â””â”€â”€ inference.py     # Sample inference script with predicted labels
â”œâ”€â”€ train.py             # Transfer learning setup (Task 3)
â”œâ”€â”€ main.py              # End-to-end execution of all tasks
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸš€ Usage

### ğŸ“Œ Install dependencies
```bash
pip install -r requirements.txt
```

### â–¶ï¸ Run all tasks
```bash
python main.py
```

### ğŸ§ª Run inference only
```bash
python -m app.inference
```

---

## ğŸ”§ Model
- Model: `bert-base-uncased`
- Framework: PyTorch + HuggingFace Transformers

---

## ğŸ“ Notes
- Predictions are printed alongside logits for interpretability.
- Dummy labels are used for demonstration â€” replace with your dataset for training.
